setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
#Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
#Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
#Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
#Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
#Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
#Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
#Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
#Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
#Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
#Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
#Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
#Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
#Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
#Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
#Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
#Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
#Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
knitr::opts_chunk$set(message = FALSE)
# Empty Global Enivronment
rm(list = ls())
# Install Package, this only need to be done once.
# install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
# install.packages('plyr', repos = "http://cran.us.r-project.org")
# Load packages, this need to be done every time you run this script.
library(dplyr)
library(tidyverse)
library(data.table)
library(knitr)
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/Measures/PKBS/DataUploadAutomation/Measures/PKBS/")
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/DataUploadAutomation/Measures/PKBS/")
# Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
UO_T2_PKBS <- read.csv("UO_T2_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T2_PKBS <- read.csv("UPMC_T2_PKBS.csv", stringsAsFactors = FALSE)
UO_T3_PKBS <- read.csv("UO_T3_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T3_PKBS <- read.csv("UPMC_T3_PKBS.csv", stringsAsFactors = FALSE)
UO_T4_PKBS <- read.csv("UO_T4_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T4_PKBS <- read.csv("UPMC_T4_PKBS.csv", stringsAsFactors = FALSE)
# Rename variables that we want to select for PKBS
# Create list of new variable names
pkbs <- "srm_pkbs"
num_items <- seq(1:33)
new_PKBS_names <- paste(pkbs, num_items, sep='_')
# Now make a list of old variable names so that we can replace them with the neww ones
UO_Q407 <- "Q407"
UO_Q359 <- "Q359"
UO_Q524 <- "Q524"
UO_Q817 <- "Q817"
UPMC_Q16 <- "Q16.1"
UPMC_Q13 <- "Q13.1"
old_UO_PKBS_names <- paste(UO_Q407, num_items,sep = "_")
old_UO_PKBS_names2 <- paste(UO_Q359, num_items,sep = "_")
old_UO_PKBS_names3 <- paste(UO_Q524, num_items,sep = "_")
old_UO_PKBS_names4 <- paste(UO_Q817, num_items,sep = "_")
old_UPMC_PKBS_names <- paste(UPMC_Q16, num_items,sep = "_")
old_UPMC_PKBS_names2 <- paste(UPMC_Q13, num_items, sep = "_")
# Change UO column names
setnames(UO_T1_PKBS, old_UO_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T2_PKBS, old_UO_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
# Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
# Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
# Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
# Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
# Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
# Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
# Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
# Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
# Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
# Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
# Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
# Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
# Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
# Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
# Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
# Empty Global Enivronment
rm(list = ls())
# Install Package, this only need to be done once.
# install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
# install.packages('plyr', repos = "http://cran.us.r-project.org")
# Load packages, this need to be done every time you run this script.
library(dplyr)
#library(tidyverse)
library(data.table)
library(knitr)
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/DataUploadAutomation/Measures/PKBS/")
# Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
# Import PKBS files from both sites and every timepoint
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
UO_T2_PKBS <- read.csv("UO_T2_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T2_PKBS <- read.csv("UPMC_T2_PKBS.csv", stringsAsFactors = FALSE)
UO_T3_PKBS <- read.csv("UO_T3_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T3_PKBS <- read.csv("UPMC_T3_PKBS.csv", stringsAsFactors = FALSE)
UO_T4_PKBS <- read.csv("UO_T4_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T4_PKBS <- read.csv("UPMC_T4_PKBS.csv", stringsAsFactors = FALSE)
# Rename variables that we want to select for PKBS
# Create list of new variable names
pkbs <- "srm_pkbs"
num_items <- seq(1:33)
new_PKBS_names <- paste(pkbs, num_items, sep='_')
# Now make a list of old variable names so that we can replace them with the neww ones
UO_Q407 <- "Q407"
UO_Q359 <- "Q359"
UO_Q524 <- "Q524"
UO_Q817 <- "Q817"
UPMC_Q16 <- "Q16.1"
UPMC_Q13 <- "Q13.1"
old_UO_PKBS_names <- paste(UO_Q407, num_items,sep = "_")
old_UO_PKBS_names2 <- paste(UO_Q359, num_items,sep = "_")
old_UO_PKBS_names3 <- paste(UO_Q524, num_items,sep = "_")
old_UO_PKBS_names4 <- paste(UO_Q817, num_items,sep = "_")
old_UPMC_PKBS_names <- paste(UPMC_Q16, num_items,sep = "_")
old_UPMC_PKBS_names2 <- paste(UPMC_Q13, num_items, sep = "_")
# Change UO column names
setnames(UO_T1_PKBS, old_UO_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T2_PKBS, old_UO_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
# Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
# Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
# Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
# Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
# Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
# Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
# Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
# Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
# Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
# Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
# Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
# Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
# Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
# Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
# Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
# save off what you have installed in homebrew
brew list > ~/Documents/currently-installed-homebrew-formulas.txt
# uninstall x86_64 homebrew
# NOTE that in theory x86_64 and arm64 homebrew can live happily together but
# I went cheap on the SSD in the M1 Mini and wld like the space back
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall.sh)"
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/PKBS_Upload_Script.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/PKBS_Upload_Script.R")
#Remove any unnecessary dataframes for the NDA upload
rm(AAQ_NDA_Prep, first_line)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/PKBS_Upload_Script.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/BearDragon_Upload_Script.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/BearDragon_Upload_Script.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
#setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
View(AAQ_NDA)
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
#Turn any -9999 response to NA
na_if(AffectPT_NDA, "-9999")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
BearDragon_NDA <- read.csv("beardragon01_template.csv")
# Select the relevant sets of information from RedCap necessary for the BearDragon
BearDragon_Prep <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint,
oc_bd_01, oc_bd_02, oc_bd_03, oc_bd_04, oc_bd_05, oc_bd_06, oc_bd_07, oc_bd_08, oc_bd_09, oc_bd_10))
View(BearDragon_Prep)
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents?GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
View(AAQ_Prep)
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
AAQ_NDA <- read.csv("acceptance01_template.csv", skip = 1)
# Select the relevant sets of information from Qualtrics necessary for the AAQ
AAQ_Prep <- select(Qualtrics, c(mom_guid, FamID_Mother, interview_date, interview_age_Mom, mother_sex, Timepoint, contains("srm_aaq")))
View(AAQ_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
AAQ_NDA <- read.csv("acceptance01_template.csv", skip = 1)
# Select the relevant sets of information from Qualtrics necessary for the AAQ
AAQ_Prep <- select(Qualtrics, c(mom_guid, FamID_Mother, interview_date, interview_age_Mom, mother_sex, Timepoint, contains("srm_aaq")))
View(AAQ_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/BearDragon_Upload_Script.R")
View(BearDragon_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
BearDragon_NDA <- read.csv("beardragon01_template.csv")
# Select the relevant sets of information from RedCap necessary for the BearDragon
BearDragon_Prep <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint,
oc_bd_01, oc_bd_02, oc_bd_03, oc_bd_04, oc_bd_05, oc_bd_06, oc_bd_07, oc_bd_08, oc_bd_09, oc_bd_10))
View(BearDragon_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/BearDragon_Upload_Script.R")
View(BearDragon_NDA)
View(BearDragon_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AffectPT_Upload_Script.R")
View(AffectPT_NDA)
View(BearDragon_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/AAQ_Upload_Script.R")
View(AAQ_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/PKBS_Upload_Script.R")
View(PKBS_NDA)
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Pedigree <- read.csv("Reference_Pedigree.csv", stringsAsFactors = FALSE)
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
```{r CBCL, warning=FALSE, message=FALSE}
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
getwd()
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
getwd()
