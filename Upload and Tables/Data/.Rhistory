<<<<<<< Updated upstream
View(WCCL_Prep)
View(NDA_WCCL)
View(NDA_WCCL_Prep)
NDA_WCCL <- bind_rows(NDA_WCCL, NDA_WCCL_Prep)
# Re-code and 67% Rule
Convert Likert Scale text input into numerical values then create a dataframe to place ID's that do not have 67% of their data present.
```{r Recode}
#Change Numbers to Numeric values
WCCL_Prep[,6:64] <- sapply(WCCL_Prep[,6:64],as.numeric)
```
```{r Calculated Columns}
# Items for SU
SU <- colnames(select(WCCL_Prep, c("srm_wccl_1", "srm_wccl_2", "srm_wccl_4", "srm_wccl_6", "srm_wccl_9", "srm_wccl_10", "srm_wccl_11", "srm_wccl_13", "srm_wccl_16", "srm_wccl_18", "srm_wccl_19", "srm_wccl_21", "srm_wccl_22", "srm_wccl_23", "srm_wccl_26", "srm_wccl_27", "srm_wccl_29", "srm_wccl_31", "srm_wccl_33", "srm_wccl_34", "srm_wccl_35", "srm_wccl_36", "srm_wccl_38", "srm_wccl_39", "srm_wccl_40", "srm_wccl_42", "srm_wccl_43", "srm_wccl_44", "srm_wccl_47", "srm_wccl_49", "srm_wccl_50", "srm_wccl_51", "srm_wccl_53", "srm_wccl_54", "srm_wccl_56", "srm_wccl_57", "srm_wccl_58", "srm_wccl_59")))
# Items for GSC
GSC <- colnames(select(WCCL_Prep, c("srm_wccl_3", "srm_wccl_5", "srm_wccl_8", "srm_wccl_12", "srm_wccl_14", "srm_wccl_17", "srm_wccl_20", "srm_wccl_25", "srm_wccl_32", "srm_wccl_37", "srm_wccl_41", "srm_wccl_45", "srm_wccl_46", "srm_wccl_52", "srm_wccl_55")))
# Items for BO
BO <- colnames(select(WCCL_Prep, c("srm_wccl_7", "srm_wccl_15", "srm_wccl_24", "srm_wccl_28", "srm_wccl_30", "srm_wccl_48")))
# Calculated Columns
WCCL_Prep$wccl_SU_raw <- rowMeans(WCCL_Prep[,SU], na.rm = TRUE)
WCCL_Prep$wccl_GSC_raw <- rowMeans(WCCL_Prep[,GSC], na.rm = TRUE)
WCCL_Prep$wccl_BO_raw <- rowMeans(WCCL_Prep[,BO], na.rm = TRUE)
# Mean with 67% Rule####
# Check NA Percentage
WCCL_Prep$NACheck <- rowSums(is.na(select(WCCL_Prep, starts_with("srm"))))/ncol(dplyr::select(WCCL_Prep, starts_with("srm")))
# New Mean with 67% Rule
WCCL_Prep$wccl_SU_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,SU], na.rm = TRUE), "NA")
WCCL_Prep$wccl_GSC_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,GSC], na.rm = TRUE), "NA")
WCCL_Prep$wccl_BO_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,BO], na.rm = TRUE), "NA")
#VarScore columns
WCCL_Prep <- add_column(WCCL_Prep, WCCL_SU_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_1", "srm_wccl_2", "srm_wccl_4", "srm_wccl_6", "srm_wccl_9", "srm_wccl_10", "srm_wccl_11", "srm_wccl_13", "srm_wccl_16", "srm_wccl_18", "srm_wccl_19", "srm_wccl_21", "srm_wccl_22", "srm_wccl_23", "srm_wccl_26", "srm_wccl_27", "srm_wccl_29", "srm_wccl_31", "srm_wccl_33", "srm_wccl_34", "srm_wccl_35", "srm_wccl_36", "srm_wccl_38", "srm_wccl_39", "srm_wccl_40", "srm_wccl_42", "srm_wccl_43", "srm_wccl_44", "srm_wccl_47", "srm_wccl_49", "srm_wccl_50", "srm_wccl_51", "srm_wccl_53", "srm_wccl_54", "srm_wccl_56", "srm_wccl_57", "srm_wccl_58", "srm_wccl_59"), MaxMiss = .20),.after = "wccl_BO_cor")
WCCL_Prep <- add_column(WCCL_Prep, WCCL_GSC_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_3", "srm_wccl_5", "srm_wccl_8", "srm_wccl_12", "srm_wccl_14", "srm_wccl_17", "srm_wccl_20", "srm_wccl_25", "srm_wccl_32", "srm_wccl_37", "srm_wccl_41", "srm_wccl_45", "srm_wccl_46", "srm_wccl_52", "srm_wccl_55"), MaxMiss = .20),.after = "wccl_BO_cor")
WCCL_Prep <- add_column(WCCL_Prep, WCCL_BO_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_7", "srm_wccl_15", "srm_wccl_24", "srm_wccl_28", "srm_wccl_30", "srm_wccl_48"), MaxMiss = .20),.after = "wccl_BO_cor")
```
```{r NDA Sheet}
# Create NDA structure column names
dbt_wccl <- paste("dbt_wccl", 1:59, sep = "")
NDA_Names <- c(dbt_wccl)
# Create NDA Prep structure
NDA_WCCL_Prep <- select(WCCL_Prep, c(subjectkey, src_subject_id, interview_date, interview_age_Mom, sex = mother_sex , visit = Timepoint, starts_with("srm")))
setnames(NDA_WCCL_Prep, new_WCCL_names, NDA_Names)
# bind NDA_WCCL_Prep and NDA structure
NDA_WCCL <- bind_rows(NDA_WCCL, NDA_WCCL_Prep)
# recreate first row of NDA structure
first_line <- matrix("", nrow = 1, ncol = ncol(NDA_WCCL))
NDA_WCCL <- bind_rows(NDA_WCCL, NDA_WCCL_Prep)
first_line <- matrix("", nrow = 1, ncol = ncol(NDA_WCCL))
first_line[,1] <- "dbt_wccl"
first_line[,2] <- "1"
write.table(first_line, file = "dbt_wccl.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = TRUE, row.names = FALSE)
write.table(NDA_WCCL, file = "dbt_wccl.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = TRUE, row.names = FALSE)
```
# Re-code and 67% Rule
Convert Likert Scale text input into numerical values then create a dataframe to place ID's that do not have 67% of their data present.
```{r Recode}
#Change Numbers to Numeric values
WCCL_Prep[,6:64] <- sapply(WCCL_Prep[,6:64],as.numeric)
```
```{r Calculated Columns}
# Items for SU
SU <- colnames(select(WCCL_Prep, c("srm_wccl_1", "srm_wccl_2", "srm_wccl_4", "srm_wccl_6", "srm_wccl_9", "srm_wccl_10", "srm_wccl_11", "srm_wccl_13", "srm_wccl_16", "srm_wccl_18", "srm_wccl_19", "srm_wccl_21", "srm_wccl_22", "srm_wccl_23", "srm_wccl_26", "srm_wccl_27", "srm_wccl_29", "srm_wccl_31", "srm_wccl_33", "srm_wccl_34", "srm_wccl_35", "srm_wccl_36", "srm_wccl_38", "srm_wccl_39", "srm_wccl_40", "srm_wccl_42", "srm_wccl_43", "srm_wccl_44", "srm_wccl_47", "srm_wccl_49", "srm_wccl_50", "srm_wccl_51", "srm_wccl_53", "srm_wccl_54", "srm_wccl_56", "srm_wccl_57", "srm_wccl_58", "srm_wccl_59")))
# Items for GSC
GSC <- colnames(select(WCCL_Prep, c("srm_wccl_3", "srm_wccl_5", "srm_wccl_8", "srm_wccl_12", "srm_wccl_14", "srm_wccl_17", "srm_wccl_20", "srm_wccl_25", "srm_wccl_32", "srm_wccl_37", "srm_wccl_41", "srm_wccl_45", "srm_wccl_46", "srm_wccl_52", "srm_wccl_55")))
# Items for BO
BO <- colnames(select(WCCL_Prep, c("srm_wccl_7", "srm_wccl_15", "srm_wccl_24", "srm_wccl_28", "srm_wccl_30", "srm_wccl_48")))
# Calculated Columns
WCCL_Prep$wccl_SU_raw <- rowMeans(WCCL_Prep[,SU], na.rm = TRUE)
WCCL_Prep$wccl_GSC_raw <- rowMeans(WCCL_Prep[,GSC], na.rm = TRUE)
WCCL_Prep$wccl_BO_raw <- rowMeans(WCCL_Prep[,BO], na.rm = TRUE)
# Mean with 67% Rule####
# Check NA Percentage
WCCL_Prep$NACheck <- rowSums(is.na(select(WCCL_Prep, starts_with("srm"))))/ncol(dplyr::select(WCCL_Prep, starts_with("srm")))
# New Mean with 67% Rule
WCCL_Prep$wccl_SU_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,SU], na.rm = TRUE), "NA")
WCCL_Prep$wccl_GSC_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,GSC], na.rm = TRUE), "NA")
WCCL_Prep$wccl_BO_cor <- ifelse(WCCL_Prep$NACheck < 0.67, rowMeans(WCCL_Prep[,BO], na.rm = TRUE), "NA")
#VarScore columns
WCCL_Prep <- add_column(WCCL_Prep, WCCL_SU_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_1", "srm_wccl_2", "srm_wccl_4", "srm_wccl_6", "srm_wccl_9", "srm_wccl_10", "srm_wccl_11", "srm_wccl_13", "srm_wccl_16", "srm_wccl_18", "srm_wccl_19", "srm_wccl_21", "srm_wccl_22", "srm_wccl_23", "srm_wccl_26", "srm_wccl_27", "srm_wccl_29", "srm_wccl_31", "srm_wccl_33", "srm_wccl_34", "srm_wccl_35", "srm_wccl_36", "srm_wccl_38", "srm_wccl_39", "srm_wccl_40", "srm_wccl_42", "srm_wccl_43", "srm_wccl_44", "srm_wccl_47", "srm_wccl_49", "srm_wccl_50", "srm_wccl_51", "srm_wccl_53", "srm_wccl_54", "srm_wccl_56", "srm_wccl_57", "srm_wccl_58", "srm_wccl_59"), MaxMiss = .20),.after = "wccl_BO_cor")
WCCL_Prep <- add_column(WCCL_Prep, WCCL_GSC_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_3", "srm_wccl_5", "srm_wccl_8", "srm_wccl_12", "srm_wccl_14", "srm_wccl_17", "srm_wccl_20", "srm_wccl_25", "srm_wccl_32", "srm_wccl_37", "srm_wccl_41", "srm_wccl_45", "srm_wccl_46", "srm_wccl_52", "srm_wccl_55"), MaxMiss = .20),.after = "wccl_BO_cor")
WCCL_Prep <- add_column(WCCL_Prep, WCCL_BO_imputation = varScore(WCCL_Prep, Forward = c("srm_wccl_7", "srm_wccl_15", "srm_wccl_24", "srm_wccl_28", "srm_wccl_30", "srm_wccl_48"), MaxMiss = .20),.after = "wccl_BO_cor")
```
```{r NDA Sheet}
# Create NDA structure column names
dbt_wccl <- paste("dbt_wccl", 1:59, sep = "")
NDA_Names <- c(dbt_wccl)
# Create NDA Prep structure
NDA_WCCL_Prep <- select(WCCL_Prep, c(subjectkey, src_subject_id, interview_date, interview_age_Mom, sex = mother_sex , visit = Timepoint, starts_with("srm")))
setnames(NDA_WCCL_Prep, new_WCCL_names, NDA_Names)
# bind NDA_WCCL_Prep and NDA structure
NDA_WCCL <- bind_rows(NDA_WCCL, NDA_WCCL_Prep)
# recreate first row of NDA structure
first_line <- matrix("", nrow = 1, ncol = ncol(NDA_WCCL))
NDA_WCCL <- bind_rows(NDA_WCCL, NDA_WCCL_Prep)
first_line <- matrix("", nrow = 1, ncol = ncol(NDA_WCCL))
first_line[,1] <- "dbt_wccl"
first_line[,2] <- "1"
write.table(first_line, file = "dbt_wccl.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = TRUE, row.names = FALSE)
write.table(NDA_WCCL, file = "dbt_wccl.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = TRUE, row.names = FALSE)
```
source("Upload Preparation.R")
source("Upload Preparation.R")
source("Upload Preparation.R")
source("Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("C:\Users\kmurr\OneDrive\Documents\GitHub\DataUploadAutomation\Measures\WCCL\Upload Preparation.R")
source("C:/Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/Measures/WCCL/Upload Preparation.R")
source("Upload Preparation.R")
source("~/Documents/kmurr/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("\\DESKTOP-JLQCFEN\Users\kmurr\OneDrive\Documents\GitHub\DataUploadAutomation\Upload and Tables\Data\Upload Preparation.R")
source(C:/Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts)
source(C:Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/UploadandTables/Scripts)
source(C:/Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/UploadandTables/Scripts)
source(C:\Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/UploadandTables/Scripts)
source('~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R')
source(C:~/Users/kmurr/OneDrive/Documents/GitHub/DataUploadAutomation/UploadandTables/Scripts)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R")
View(NDA_WCCL)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/CBCL_Upload_Script.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R")
View(WCCL_NDA)
# import data frame
#setwd("c:/GitHub/DataUploadAutomation/Upload and Tables/Data")
#source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
#setwd
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
# import data frame
#setwd("C:/GitHub")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
#setwd("c:/GitHub/DataUploadAutomation/Upload and Tables/Data")
#source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
# import data frame
#setwd("C:/GitHub")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Setup
#setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
# import data frame
WCCL_NDA <- read.csv("dbt_wccl01_template.csv", skip = 1)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R")
# Setup
#setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/WCCL_Upload_Script.R")
View(WCCL_Prep)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/EL_Upload_Script.R")
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
View(Redcap_Data)
View(Qualtrics)
View(Pedigree)
View(Pedigree)
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
# add mean columns
EL_PREP$oc_elt_exp_total <- rowMeans(EL_PREP[,exp], na.rm = TRUE)
View(EL_PREP)
exp <- colnames(select(EL_PREP, c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8")))
rec <- colnames(select(EL_PREP, c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4")))
# add mean columns
EL_PREP$oc_elt_exp_total <- rowMeans(EL_PREP[,exp], na.rm = TRUE)
EL_PREP$oc_elt_rec_total <- rowMeans(EL_PREP[,rec], na.rm = TRUE)
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in Emotion_Ladeling_NDA into el.csv file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in Emotion_Ladeling_NDA into el.csv file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
View(EL_PREP)
View(EL_PREP)
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
View(EL_PREP)
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in Emotion_Ladeling_NDA into el.csv file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in Emotion_Ladeling_NDA into el.csv file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
# Append data in Emotion_Ladeling_NDA into el.csv file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
# NDA output ---------
# Create a new file in folder called el.csv, and put first line into this file
# el.csv file will be saved into same folder as current r script
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
View(EL_PREP)
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
View(EL_NDA_Prep)
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
View(Emotion_Labeling_NDA)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
EL1_Names <- c(oc_elt_exp)
EL1_Names <- c("oc_elt_exp")
EL2_Names <- c("oc_elt_rec")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
EL1_Names <- c("oc_elt_exp_")
EL1_Names <- c("oc_elt_exp_1, oc_elt_exp_2, oc_elt_exp_3, oc_elt_exp_4, oc_elt_exp_5, oc_elt_exp_6, oc_elt_exp_7, oc_elt_exp_8")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
EL1_Names <- c("oc_elt_exp_1", "oc_elt_exp_2", "oc_elt_exp_3", "oc_elt_exp_4", "oc_elt_exp_5", "oc_elt_exp_6", "oc_elt_exp_7", "oc_elt_exp_8")
EL1_Names <- c("oc_elt_exp_1", "oc_elt_exp_2", "oc_elt_exp_3", "oc_elt_exp_4", "oc_elt_exp_5", "oc_elt_exp_6", "oc_elt_exp_7", "oc_elt_exp_8")
EL2_Names <- c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
View(EL_NDA_Prep)
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
#NDA output tests
# NDA output ####
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in WCCL_NDA into dbt_wccl.cav file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
View(Emotion_Labeling_NDA)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
View(Emotion_Labeling_NDA)
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
View(EL_NDA_Prep)
EL1_Names <- c("eltpart1_exp1", "eltpart1_exp2", "eltpart1_exp3", "eltpart1_exp4", "eltpart1_exp5", "eltpart1_exp6", "eltpart1_exp7", "eltpart1_exp8")
EL2_Names <- c("eltpart2_rec1", "eltpart2_rec2", "eltpart2_rec3", "eltpart2_rec4")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
View(EL_NDA_Prep)
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
#NDA output tests
# NDA output ####
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in WCCL_NDA into dbt_wccl.cav file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
View(EL_NDA_Prep)
View(Emotion_Labeling_NDA)
View(Emotion_Labeling_NDA)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
View(Emotion_Labeling_NDA)
View(EL_PREP)
EL1_Names <- c("eltpart1_exp1", "eltpart1_exp2", "eltpart1_exp3", "eltpart1_exp4", "eltpart1_exp5", "eltpart1_exp6", "eltpart1_exp7", "eltpart1_exp8")
EL2_Names <- c("eltpart2_rec1", "eltpart2_rec2", "eltpart2_rec3", "eltpart2_rec4")
View(EL_NDA_Prep)
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
View(EL_NDA_Prep)
View(Emotion_Labeling_NDA)
View(EL_NDA_Prep)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
#EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
#EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
#EL1_Names <- c("eltpart1_exp1", "oc_elt_exp_2", "oc_elt_exp_3", "oc_elt_exp_4", "oc_elt_exp_5", "oc_elt_exp_6", "oc_elt_exp_7", "oc_elt_exp_8")
#EL2_Names <- c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4")
EL1_Names <- c("eltpart1_exp1", "eltpart1_exp2", "eltpart1_exp3", "eltpart1_exp4", "eltpart1_exp5", "eltpart1_exp6", "eltpart1_exp7", "eltpart1_exp8")
EL2_Names <- c("eltpart2_rec1", "eltpart2_rec2", "eltpart2_rec3", "eltpart2_rec4")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
View(EL_NDA_Prep)
View(Emotion_Labeling_NDA)
View(EL_NDA_Prep)
EL_NDA_Prep <- rm("oc_elt_notes")
EL_NDA_Prep <- rm('oc_elt_notes')
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
#EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
#EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
View(EL_NDA_Prep)
EL_NDA_Prep$oc_elt_notes <- NULL
View(EL_NDA_Prep)
EL1_Names <- c("eltpart1_exp1", "eltpart1_exp2", "eltpart1_exp3", "eltpart1_exp4", "eltpart1_exp5", "eltpart1_exp6", "eltpart1_exp7", "eltpart1_exp8")
EL2_Names <- c("eltpart2_rec1", "eltpart2_rec2", "eltpart2_rec3", "eltpart2_rec4")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
#NDA output tests
# NDA output ####
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in WCCL_NDA into dbt_wccl.cav file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
source("~/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
# Title: EL Upload Script
#Setwd
setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
Emotion_Labeling_NDA <- read.csv("elt01_template.csv", skip=1)
options(digits = 3)
#EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, GroupAssignment, Timepoint, starts_with("oc_elt_")))
EL_PREP <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint, starts_with("oc_elt_")))
#Calculated Columns
EL_PREP <- add_column(EL_PREP, oc_elt_exp_total = varScore(EL_PREP, Forward = c("oc_elt_exp_2", "oc_elt_exp_4", "oc_elt_exp_6", "oc_elt_exp_8"), MaxMiss = .20),.after = "oc_elt_notes")
EL_PREP <- add_column(EL_PREP, oc_elt_rec_total = varScore(EL_PREP, Forward = c("oc_elt_rec_1", "oc_elt_rec_2", "oc_elt_rec_3", "oc_elt_rec_4"), MaxMiss = .20),.after = "oc_elt_exp_total")
# Remove -9999s
EL_PREP[EL_PREP == -9999] <- NA
# Move relevant info to NDA dataframe
EL_NDA_Prep <- select(EL_PREP, c(subjectkey = child_guid, src_subject_id = child_famID, interview_date, interview_age = interview_age_child, sex = child_sex, visit = Timepoint, starts_with("oc_elt_")))
#Remove unnecessary columns
EL_NDA_Prep$oc_elt_notes <- NULL
EL_NDA_Prep$oc_elt_rec_total <- NULL
EL_NDA_Prep$oc_elt_exp_total <- NULL
#create NDA column Names
EL1_Names <- c("eltpart1_exp1", "eltpart1_exp2", "eltpart1_exp3", "eltpart1_exp4", "eltpart1_exp5", "eltpart1_exp6", "eltpart1_exp7", "eltpart1_exp8")
EL2_Names <- c("eltpart2_rec1", "eltpart2_rec2", "eltpart2_rec3", "eltpart2_rec4")
setnames(EL_NDA_Prep, new_eltpart1_names, EL1_Names)
setnames(EL_NDA_Prep, new_eltpart2_names, EL2_Names)
View(Emotion_Labeling_NDA)
# Recreate first line in orignial NDA file
Emotion_Labeling_NDA <- bind_rows(mutate_all(Emotion_Labeling_NDA, as.character), mutate_all(EL_NDA_Prep, as.character))
View(Emotion_Labeling_NDA)
first_line <- matrix("", nrow = 1, ncol = ncol(Emotion_Labeling_NDA))
# assign the second cell in first_line as "el"
first_line[,1] <- "el"
first_line[,2] <- "1"
#NDA output tests
# NDA output ####
write.table(first_line, file = "NDA Upload/el01.csv", sep = ",", append = FALSE, quote = FALSE, na = "", col.names = FALSE, row.names = FALSE)
# Append data in WCCL_NDA into dbt_wccl.cav file
write.table(Emotion_Labeling_NDA, file = 'NDA Upload/el01.csv', sep = ",", append = TRUE, na = "", quote = FALSE, row.names = FALSE)
=======
knitr::opts_chunk$set(message = FALSE)
# Empty Global Enivronment
rm(list = ls())
#Install Package, this only need to be done once.
#install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
#install.packages('plyr', repos = "http://cran.us.r-project.org")
#Load packages, this need to be done every time you run this script.
library(dplyr)
library(tidyverse)
library(data.table)
library(knitr)
#Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/Measures/PKBS/DataUploadAutomation/Measures/PKBS/")
#Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
#Import PKBS files from both sites and every timepoint
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
#Import PKBS files from both sites and every timepoint
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
knitr::opts_chunk$set(message = FALSE)
# Empty Global Enivronment
rm(list = ls())
Install Package, this only need to be done once.
# Empty Global Enivronment
rm(list = ls())
#Install Package, this only need to be done once.
install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
install.packages('plyr', repos = "http://cran.us.r-project.org")
#Load packages, this need to be done every time you run this script.
library(dplyr)
library(tidyverse)
install.packages('plyr', repos = "http://cran.us.r-project.org")
library(dplyr)
library(tidyverse)
library(data.table)
library(knitr)
#Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/Measures/PKBS/DataUploadAutomation/Measures/PKBS/")
#Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
UO_T2_PKBS <- read.csv("UO_T2_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T2_PKBS <- read.csv("UPMC_T2_PKBS.csv", stringsAsFactors = FALSE)
UO_T3_PKBS <- read.csv("UO_T3_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T3_PKBS <- read.csv("UPMC_T3_PKBS.csv", stringsAsFactors = FALSE)
UO_T4_PKBS <- read.csv("UO_T4_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T4_PKBS <- read.csv("UPMC_T4_PKBS.csv", stringsAsFactors = FALSE)
#Rename variables that we want to select for PKBS
#Create list of new variable names
pkbs <- "srm_pkbs"
num_items <- seq(1:33)
new_PKBS_names <- paste(pkbs, num_items, sep='_')
#Now make a list of old variable names so that we can replace them with the neww ones
UO_Q407 <- "Q407"
UO_Q359 <- "Q359"
UO_Q524 <- "Q524"
UO_Q817 <- "Q817"
UPMC_Q16 <- "Q16.1"
UPMC_Q13 <- "Q13.1"
old_UO_PKBS_names <- paste(UO_Q407, num_items,sep = "_")
old_UO_PKBS_names2 <- paste(UO_Q359, num_items,sep = "_")
old_UO_PKBS_names3 <- paste(UO_Q524, num_items,sep = "_")
old_UO_PKBS_names4 <- paste(UO_Q817, num_items,sep = "_")
old_UPMC_PKBS_names <- paste(UPMC_Q16, num_items,sep = "_")
old_UPMC_PKBS_names2 <- paste(UPMC_Q13, num_items, sep = "_")
#Change UO column names
setnames(UO_T1_PKBS, old_UO_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T2_PKBS, old_UO_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
#Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
#Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
#Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
#Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
#Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
#Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
#Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
#Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
#Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
#Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
#Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
#Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
#Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
#Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
#Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
#Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
#Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
knitr::opts_chunk$set(message = FALSE)
# Empty Global Enivronment
rm(list = ls())
# Install Package, this only need to be done once.
# install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
# install.packages('plyr', repos = "http://cran.us.r-project.org")
# Load packages, this need to be done every time you run this script.
library(dplyr)
library(tidyverse)
library(data.table)
library(knitr)
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/Measures/PKBS/DataUploadAutomation/Measures/PKBS/")
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/DataUploadAutomation/Measures/PKBS/")
# Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
UO_T2_PKBS <- read.csv("UO_T2_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T2_PKBS <- read.csv("UPMC_T2_PKBS.csv", stringsAsFactors = FALSE)
UO_T3_PKBS <- read.csv("UO_T3_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T3_PKBS <- read.csv("UPMC_T3_PKBS.csv", stringsAsFactors = FALSE)
UO_T4_PKBS <- read.csv("UO_T4_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T4_PKBS <- read.csv("UPMC_T4_PKBS.csv", stringsAsFactors = FALSE)
# Rename variables that we want to select for PKBS
# Create list of new variable names
pkbs <- "srm_pkbs"
num_items <- seq(1:33)
new_PKBS_names <- paste(pkbs, num_items, sep='_')
# Now make a list of old variable names so that we can replace them with the neww ones
UO_Q407 <- "Q407"
UO_Q359 <- "Q359"
UO_Q524 <- "Q524"
UO_Q817 <- "Q817"
UPMC_Q16 <- "Q16.1"
UPMC_Q13 <- "Q13.1"
old_UO_PKBS_names <- paste(UO_Q407, num_items,sep = "_")
old_UO_PKBS_names2 <- paste(UO_Q359, num_items,sep = "_")
old_UO_PKBS_names3 <- paste(UO_Q524, num_items,sep = "_")
old_UO_PKBS_names4 <- paste(UO_Q817, num_items,sep = "_")
old_UPMC_PKBS_names <- paste(UPMC_Q16, num_items,sep = "_")
old_UPMC_PKBS_names2 <- paste(UPMC_Q13, num_items, sep = "_")
# Change UO column names
setnames(UO_T1_PKBS, old_UO_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T2_PKBS, old_UO_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
# Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
# Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
# Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
# Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
# Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
# Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
# Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
# Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
# Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
# Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
# Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
# Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
# Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
# Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
# Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
# Empty Global Enivronment
rm(list = ls())
# Install Package, this only need to be done once.
# install.packages(c("dplyr","tidyverse","data.table","contrib.url","knitr"))
# install.packages('plyr', repos = "http://cran.us.r-project.org")
# Load packages, this need to be done every time you run this script.
library(dplyr)
#library(tidyverse)
library(data.table)
library(knitr)
# Set Working Directory
setwd("~/Documents/GitHub/DataUploadAutomation/DataUploadAutomation/Measures/PKBS/")
# Import Pedigree and NDA Structure
Pedigree <- read.csv("Reference_Pedigree.csv")
NDA_PKBS <- read.csv("pkbs01_template.csv", skip = 1)
# Import PKBS files from both sites and every timepoint
UO_T1_PKBS <- read.csv("UO_T1_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T1_PKBS <- read.csv("UPMC_T1_PKBS.csv", stringsAsFactors = FALSE)
UO_T2_PKBS <- read.csv("UO_T2_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T2_PKBS <- read.csv("UPMC_T2_PKBS.csv", stringsAsFactors = FALSE)
UO_T3_PKBS <- read.csv("UO_T3_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T3_PKBS <- read.csv("UPMC_T3_PKBS.csv", stringsAsFactors = FALSE)
UO_T4_PKBS <- read.csv("UO_T4_Qualtrics.csv", stringsAsFactors = FALSE)
UPMC_T4_PKBS <- read.csv("UPMC_T4_PKBS.csv", stringsAsFactors = FALSE)
# Rename variables that we want to select for PKBS
# Create list of new variable names
pkbs <- "srm_pkbs"
num_items <- seq(1:33)
new_PKBS_names <- paste(pkbs, num_items, sep='_')
# Now make a list of old variable names so that we can replace them with the neww ones
UO_Q407 <- "Q407"
UO_Q359 <- "Q359"
UO_Q524 <- "Q524"
UO_Q817 <- "Q817"
UPMC_Q16 <- "Q16.1"
UPMC_Q13 <- "Q13.1"
old_UO_PKBS_names <- paste(UO_Q407, num_items,sep = "_")
old_UO_PKBS_names2 <- paste(UO_Q359, num_items,sep = "_")
old_UO_PKBS_names3 <- paste(UO_Q524, num_items,sep = "_")
old_UO_PKBS_names4 <- paste(UO_Q817, num_items,sep = "_")
old_UPMC_PKBS_names <- paste(UPMC_Q16, num_items,sep = "_")
old_UPMC_PKBS_names2 <- paste(UPMC_Q13, num_items, sep = "_")
# Change UO column names
setnames(UO_T1_PKBS, old_UO_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T2_PKBS, old_UO_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T3_PKBS, old_UO_PKBS_names3, new_PKBS_names, skip_absent=FALSE)
setnames(UO_T4_PKBS, old_UO_PKBS_names4, new_PKBS_names, skip_absent=FALSE)
# Change UPMC column names
setnames(UPMC_T1_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T2_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T3_PKBS, old_UPMC_PKBS_names2, new_PKBS_names, skip_absent=FALSE)
setnames(UPMC_T4_PKBS, old_UPMC_PKBS_names, new_PKBS_names, skip_absent=FALSE)
# Edit UO PKBS Times 1-4 to have only PKBS questions and the FamID
UO_T1_PKBS <- select(UO_T1_PKBS, c(FamID = Q221, contains("pkbs")))
UO_T2_PKBS <- select(UO_T2_PKBS, c(FamID = Q116, contains("pkbs")))
UO_T3_PKBS <- select(UO_T3_PKBS, c(FamID = Q174, contains("pkbs")))
UO_T4_PKBS <- select(UO_T4_PKBS, c(FamID = Q203, contains("pkbs")))
# Edit UPMC PKBS Times 1-4 to have only PKBS questions and the FamID
UPMC_T1_PKBS <- select(UPMC_T1_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T2_PKBS <- select(UPMC_T2_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T3_PKBS <- select(UPMC_T3_PKBS, c(FamID = Q1.2, contains("pkbs")))
UPMC_T4_PKBS <- select(UPMC_T4_PKBS, c(FamID = Q1.2, contains("pkbs")))
# Merge UO and UPMC data by Timepoint
PKBS_T1 <- rbind(UO_T1_PKBS, UPMC_T1_PKBS)
PKBS_T2 <- rbind(UO_T2_PKBS, UPMC_T2_PKBS)
PKBS_T3 <- rbind(UO_T3_PKBS, UPMC_T3_PKBS)
PKBS_T4 <- rbind(UO_T4_PKBS, UPMC_T4_PKBS)
# Clean up environment
rm(UO_T1_PKBS, UO_T2_PKBS, UO_T3_PKBS, UO_T4_PKBS, UPMC_T1_PKBS, UPMC_T2_PKBS, UPMC_T3_PKBS, UPMC_T4_PKBS)
# Create the Pedigree table for each timepoint
Pedigree_T1 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time1Date, MomAge_T1, GroupAssignment)
Pedigree_T2 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time2Date, MomAge_T2, GroupAssignment)
Pedigree_T3 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time3Date, MomAge_T3, GroupAssignment)
Pedigree_T4 <- select(Pedigree, FamID, FamID_Mother, mom_guid, MomGender, Time4Date, MomAge_T4, GroupAssignment)
# Merge Pedigree data to PKBS timepoints
PKBS_T1 <- merge(Pedigree_T1, PKBS_T1, by = "FamID")
PKBS_T2 <- merge(Pedigree_T2, PKBS_T2, by = "FamID")
PKBS_T3 <- merge(Pedigree_T3, PKBS_T3, by = "FamID")
PKBS_T4 <- merge(Pedigree_T4, PKBS_T4, by = "FamID")
# Clean up environment
rm(Pedigree, Pedigree_T1, Pedigree_T2, Pedigree_T3, Pedigree_T4)
# Create new column designating time point for each database
PKBS_T1$Timepoint <- "Time 1"
PKBS_T2$Timepoint <- "Time 2"
PKBS_T3$Timepoint <- "Time 3"
PKBS_T4$Timepoint <- "Time 4"
# Rename each of the Date and Age columns so that they match
PKBS_T1 <- PKBS_T1 %>% rename( interview_date = Time1Date, interview_age = MomAge_T1)
PKBS_T2 <- PKBS_T2 %>% rename( interview_date = Time2Date, interview_age = MomAge_T2)
PKBS_T3 <- PKBS_T3 %>% rename( interview_date = Time3Date, interview_age = MomAge_T3)
PKBS_T4 <- PKBS_T4 %>% rename( interview_date = Time4Date, interview_age = MomAge_T4)
# Merge all timepoints together to create the PKBS prep sheet
PKBS_Prep <- rbind(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Change gender to F instead of FALSE
PKBS_Prep$MomGender <- "F"
# Clean up environment
rm(PKBS_T1, PKBS_T2, PKBS_T3, PKBS_T4)
# Turn Likert Scale from text string to numeric value
PKBS_Prep[PKBS_Prep == "Never (0)"] <- 0; PKBS_Prep[PKBS_Prep == "Rarely (1)"] <- 1;
PKBS_Prep[PKBS_Prep == "Sometimes (2)"] <- 2; PKBS_Prep[PKBS_Prep == "Often (3)"] <- 3;
# Create NA Check column calculating what portion of the data is present
PKBS_Prep$NACheck <- rowSums(is.na(select(PKBS_Prep, starts_with("srm_pkbs"))))/ncol(dplyr::select(PKBS_Prep, starts_with("srm_pkbs")))
# Create Drop dataframe for anything less than 0.67; Allow anything over to be used for Prep sheet
PKBS_Drop <- PKBS_Prep[PKBS_Prep$NACheck > 0.67, ]
PKBS_Prep <- PKBS_Prep[PKBS_Prep$NACheck <= 0.67, ]
# Create dataframe for all ID's that have 100% of their data present
PKBS_100 <- PKBS_Prep[PKBS_Prep$NACheck == 0, ]
# Change number to numeric values and Create Calculated Column
PKBS_Prep[,8:40] <- sapply(PKBS_Prep[,8:40],as.numeric)
PKBS_Prep  <- add_column(PKBS_Prep, pkbs_total = rowSums(PKBS_Prep[, c("srm_pkbs_1", "srm_pkbs_2","srm_pkbs_3","srm_pkbs_4", "srm_pkbs_5", "srm_pkbs_6",
"srm_pkbs_7", "srm_pkbs_8", "srm_pkbs_9", "srm_pkbs_10", "srm_pkbs_11", "srm_pkbs_12", "srm_pkbs_13",
"srm_pkbs_14", "srm_pkbs_15", "srm_pkbs_16", "srm_pkbs_17", "srm_pkbs_18", "srm_pkbs_19", "srm_pkbs_20",
"srm_pkbs_21", "srm_pkbs_22", "srm_pkbs_23", "srm_pkbs_24", "srm_pkbs_25", "srm_pkbs_26", "srm_pkbs_27",
"srm_pkbs_28", "srm_pkbs_29", "srm_pkbs_30", "srm_pkbs_31", "srm_pkbs_32", "srm_pkbs_33")]),.after = "srm_pkbs_33")
# save off what you have installed in homebrew
brew list > ~/Documents/currently-installed-homebrew-formulas.txt
# uninstall x86_64 homebrew
# NOTE that in theory x86_64 and arm64 homebrew can live happily together but
# I went cheap on the SSD in the M1 Mini and wld like the space back
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall.sh)"
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Data/Upload Preparation.R")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
# Source data, templates and create NDA dataframe
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
#setwd("~/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
setwd("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Data")
source("~/DocumentsGitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/BearDragon_Upload_Script.R")
View(BearDragon_NDA)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
View(Pedigree)
View(Qualtrics)
View(Redcap_Data)
BearDragon_NDA <- read.csv("beardragon01_template.csv", na.strings = -9999)
# Select the relevant sets of information from RedCap necessary for the BearDragon
BearDragon_Prep <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint,
oc_bd_01, oc_bd_02, oc_bd_03, oc_bd_04, oc_bd_05, oc_bd_06, oc_bd_07, oc_bd_08, oc_bd_09, oc_bd_10))
View(BearDragon_Prep)
# Replace M/F response values from child_sex column to male/female
BearDragon_Prep[BearDragon_Prep$child_sex = "M"] <- "male"
# Replace M/F response values from child_sex column to male/female
BearDragon_Prep[BearDragon_Prep$child_sex == "M"] <- "male"
View(BearDragon_Prep)
View(BearDragon_Prep)
source("~/Documents/GitHub/DataUploadAutomation/Upload and Tables/Scripts/Upload Preparation.R")
BearDragon_NDA <- read.csv("beardragon01_template.csv", na.strings = -9999)
# Select the relevant sets of information from RedCap necessary for the BearDragon
BearDragon_Prep <- select(Redcap_Data, c(child_guid, child_famID, interview_date, interview_age_child, child_sex, Timepoint,
oc_bd_01, oc_bd_02, oc_bd_03, oc_bd_04, oc_bd_05, oc_bd_06, oc_bd_07, oc_bd_08, oc_bd_09, oc_bd_10))
View(BearDragon_Prep)
# Replace M/F response values from child_sex column to male/female
BearDragon_Prep$child_sex[BearDragon_Prep$child_sex == "M"] <- "male"
View(BearDragon_Prep)
View(Pedigree)
View(Redcap_Data)
>>>>>>> Stashed changes
